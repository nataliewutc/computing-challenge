{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn import svm\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from typing import Union\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from IPython.display import display\n",
    "from time import time\n",
    "import matplotlib.widgets\n",
    "from matplotlib.widgets import RadioButtons, CheckButtons\n",
    "%matplotlib nbagg \n",
    "import matplotlib.animation \n",
    "from sklearn.inspection import permutation_importance\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing \n",
    "\n",
    "In order to clean the data provided, each of the different features were researched and considered. Ultimately, we have decided to remove the 'In literature', 'Compound', 'v(A)', 'v(B)' and 'tau'. The 'In literature' feature was removed because it didn't give useful information for the crystal structure. Then, the 'Compound' feature was removed because 'A' and 'B' would already define the elements present in the compound. Lastly, 'v(A)' and 'v(B)' which were the oxidation states were removed due to a considerable amount of missing values, and as a result the ‘tau’ column has too been removed. This is because the ‘tau’ value is one calculated based off of the oxidation state values and therefore has the same issues. An oxidation state of zero is also unreasonable because this only occurs when an element is a neutral substance containing only atoms of one element.\n",
    "\n",
    "Next, the 'A' and 'B' features were One Hot encoded to transform the categorical data. In order to avoid a hierarchical ordering, the One Hot Encoder was chosen. For this encoding method, each class is replaced by a boolean column, instead of simply being ranked by an integer.\n",
    "\n",
    "Ultimately, after the appropriate data had been selected, a scaler was applied. It was determined that the min-max scaler was the most appropriate. This is because the standard scaler has a variance of 1.0, and therefore the limits cannot be known beforehand. This is not the case with the min-max scaler, and is why that it has been chosen to be used for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning \n",
    "data = pd.read_csv('Crystal_structure.csv')\n",
    "data_to_clean = data.copy()\n",
    "data_without_columns = data_to_clean.drop(columns= ['In literature','v(A)','v(B)','τ', 'Compound'])\n",
    "data_without_rows = data_without_columns.dropna()\n",
    "data_replace_dash = data_without_rows.replace('-',np.nan)\n",
    "data_replace_zero = data_replace_dash.replace(0, np.nan)\n",
    "data_replace = data_replace_zero.dropna()\n",
    "\n",
    "# Encoding \n",
    "class Encoder:\n",
    "    def __init__(self, kind: str = 'onehot'):\n",
    "        # make sure kind is either onehot or label\n",
    "        assert kind in ['onehot', 'label']\n",
    "        self.kind = kind\n",
    "        \n",
    "    def encode(self, data: pd.Series) -> Union[pd.DataFrame, pd.Series]:\n",
    "        if self.kind == 'onehot':\n",
    "            categories = set(data)\n",
    "            new = pd.DataFrame()\n",
    "            for column in list(categories):\n",
    "                new[column] = data.apply(lambda x: 1 if x == column else 0)\n",
    "        else:\n",
    "            categories = list(set(data))\n",
    "            new = data.apply(lambda x: categories.index(x))\n",
    "            new = pd.DataFrame(new)\n",
    "            del new[column]\n",
    "        return new\n",
    "\n",
    "\n",
    "ohe_encoded = data_replace.copy()\n",
    "a = Encoder('onehot').encode(ohe_encoded['A'])\n",
    "b = Encoder('onehot').encode(ohe_encoded['B'])\n",
    "del ohe_encoded['A']\n",
    "del ohe_encoded['B']\n",
    "ohe_encoded = pd.concat([ohe_encoded, a], axis=1)\n",
    "ohe_encoded = pd.concat([ohe_encoded, b], axis=1)\n",
    "\n",
    "\n",
    "# Min-max scaling \n",
    "def min_max_scaling(data, min_idx=0, max_idx=10):\n",
    "    data_minmax = data.copy()\n",
    "    name_columns = list(data_minmax.columns)\n",
    "    for i in range(min_idx, max_idx):  \n",
    "        min_value = min(data_minmax[name_columns[i]])\n",
    "        max_value = max(data_minmax[name_columns[i]])\n",
    "        diff = int(max_value) - int(min_value)\n",
    "        data_minmax[name_columns[i]] = data_minmax[name_columns[i]].apply(lambda x: (x - min_value) / diff) \n",
    "    return data_minmax\n",
    "\n",
    "data_minmax = min_max_scaling(ohe_encoded, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, testing and classifiers\n",
    "\n",
    "The data was divided into training and testing sets with the most common split being 70/30, respectively. A higher portion for the training set was used in large data set, therefore, we chose a 75/25 split. In addition, random_state shuffles the data around and is commonly set to 42, which is we decided to use. \n",
    "\n",
    "Four differing classifications were tested to then find the best classifier, namely: multinomial logistic regression, random forest, k nearest neighbours and support vectors machine. They were compared using the score function, which determined how well the data was fitted.\n",
    "\n",
    "For logistic regression and random forest, the LogisticRegression() and RandomForestClassifier() classes of the scikit-learn library were used. For K nearest neighbours regression, the scores of the different values of K were determined and the K value with the highest score was used. The range was put up to slightly higher than the number of data points as this is commonly seen as the ideal K value. Lastly, for the support vector machine, both the linear and the kernel versions were used to find the optimal classifier. A C value of 10 was chosen to obtain a smoother decision surface. Gamma was chosen to be 1, which is an intermediate value. A lower value would cause a lower accuracy and higher would be at risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Score: 63.35\n",
      "Random Forest Score: 78.80\n",
      "KNN Score: 75.92\n",
      "SVM Score: 74.35\n"
     ]
    }
   ],
   "source": [
    "# Test-train split \n",
    "name_columns = list(data_minmax.columns)\n",
    "X = data_minmax[name_columns[:10]].to_numpy()\n",
    "y = data_minmax[name_columns[10]].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 42)\n",
    "\n",
    "# Logistic regression \n",
    "logistic = LogisticRegression(max_iter = 1000).fit(X_train, y_train)\n",
    "logistic_y_pred = logistic.predict(X_test)\n",
    "logistic_score = logistic.score(X_test, y_test)\n",
    "print('Logistic Regression Score: %.2f' % (logistic_score*100))\n",
    "\n",
    "#Random forest \n",
    "forest = RandomForestClassifier().fit(X_train,y_train)\n",
    "forest_y_pred = forest.predict(X_test)\n",
    "forest_score = forest.score(X_test, y_test)\n",
    "print('Random Forest Score: %.2f' % (forest_score*100))\n",
    "\n",
    "#KNN\n",
    "models = []\n",
    "k_values = list(range(1,70))\n",
    "# Testing different k values \n",
    "for k in k_values:\n",
    "    model= KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "scores = [model.score(X_test, y_test) for model in models]\n",
    "# Find k value with highest score \n",
    "highest_score = max(scores)\n",
    "index = scores.index(highest_score)\n",
    "# Train KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k_values[index]).fit(X_train, y_train)\n",
    "knn_y_pred = knn_model.predict(X_test)\n",
    "knn_score = knn_model.score(X_test, y_test)\n",
    "print('KNN Score: %.2f' % (knn_score*100))\n",
    "\n",
    "#SVM \n",
    "C = 10.0\n",
    "# Testing different SVM models \n",
    "models = (\n",
    "    svm.SVC(kernel='linear', C=C),\n",
    "    svm.LinearSVC(C=C, max_iter=10000),\n",
    "    svm.SVC(kernel='rbf', gamma=1, C=C),\n",
    "    svm.SVC(kernel='poly', degree=1.5, gamma='auto', C=C)\n",
    ")\n",
    "models = [clf.fit(X_train, y_train) for clf in models]\n",
    "scores = [clf.score(X_test, y_test) for clf in models]\n",
    "# Get SVM model with highest score \n",
    "highest_score = max(scores)\n",
    "index = scores.index(highest_score)\n",
    "# Train SVM model \n",
    "svm_y_pred = models[index].predict(X_test)\n",
    "svm_score = models[index].score(X_test, y_test)\n",
    "print('SVM Score: %.2f' % (svm_score*100))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export models and data to be used in next notebook\n",
    "forest_filename = 'forest_model.sav'\n",
    "pickle.dump(forest, open(forest_filename, 'wb'))\n",
    "pd.DataFrame(X_test).to_csv(\"X_test.csv\")\n",
    "pd.DataFrame(y_test).to_csv(\"y_test.csv\")\n",
    "pd.DataFrame(X_train).to_csv(\"X_train.csv\")\n",
    "pd.DataFrame(y_train).to_csv(\"y_train.csv\")\n",
    "pd.DataFrame(forest_y_pred).to_csv(\"forest_y_pred.csv\")\n",
    "pd.DataFrame(knn_y_pred).to_csv(\"knn_y_pred.csv\")\n",
    "pd.DataFrame(X).to_csv(\"X.csv\")\n",
    "pd.DataFrame(y).to_csv(\"y.csv\")\n",
    "pd.DataFrame(name_columns).to_csv(\"name_columns.csv\")\n",
    "data_minmax.to_csv(\"data_minmax.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
